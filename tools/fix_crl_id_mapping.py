#!/usr/bin/env python3
#===========================================================
# PharmaKG CRL IDæ˜ å°„ä¿®å¤å·¥å…·
# Pharmaceutical Knowledge Graph - CRL ID Mapping Fix
#===========================================================
# ç‰ˆæœ¬: v1.0
# æè¿°: ä¿®å¤CRLå…³ç³»IDä¸å®ä½“primary_idä¸åŒ¹é…çš„é—®é¢˜
#===========================================================

import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s: %(message)s'
)
logger = logging.getLogger(__name__)


class CRLIDMappingFixer:
    """
    CRLæ•°æ®IDæ˜ å°„ä¿®å¤å™¨

    ä¿®å¤å…³ç³»IDä¸å®ä½“primary_idä¸åŒ¹é…çš„é—®é¢˜
    """

    def __init__(self, entities_file: Path, relationships_file: Path):
        self.entities_file = entities_file
        self.relationships_file = relationships_file

        # åŠ è½½æ•°æ®
        with open(entities_file, 'r', encoding='utf-8') as f:
            self.entities = json.load(f)

        with open(relationships_file, 'r', encoding='utf-8') as f:
            self.relationships = json.load(f)

        logger.info(f"åŠ è½½ {len(self.entities)} ä¸ªå®ä½“, {len(self.relationships)} ä¸ªå…³ç³»")

        # IDæ˜ å°„è¡¨
        self.id_map: Dict[str, str] = {}
        self.fix_stats = defaultdict(int)

    def _build_id_mapping(self) -> None:
        """æ„å»ºIDæ˜ å°„è¡¨"""
        logger.info("æ„å»ºIDæ˜ å°„è¡¨...")

        # æ”¶é›†æ‰€æœ‰å®ä½“primary_id
        entity_ids = set()
        for entity in self.entities:
            props = entity.get('properties', {})
            primary_id = props.get('primary_id', '')
            if primary_id:
                entity_ids.add(primary_id)

        # ä¸ºæ¯ä¸ªå…³ç³»IDæŸ¥æ‰¾åŒ¹é…çš„å®ä½“ID
        all_rel_ids = set()
        for rel in self.relationships:
            all_rel_ids.add(rel.get('from', ''))
            all_rel_ids.add(rel.get('to', ''))

        # æ„å»ºæ˜ å°„ï¼šrelationship_id -> entity_id
        for rel_id in all_rel_ids:
            if not rel_id:
                continue

            # å¦‚æœç›´æ¥åŒ¹é…ï¼Œè®°å½•
            if rel_id in entity_ids:
                self.id_map[rel_id] = rel_id
                continue

            # å°è¯•æ—¥æœŸæ ¼å¼è½¬æ¢ï¼š/ -> -
            if '/' in rel_id:
                normalized = rel_id.replace('/', '-')
                if normalized in entity_ids:
                    self.id_map[rel_id] = normalized
                    self.fix_stats['date_slash_to_dash'] += 1
                    continue

            # å°è¯•æ—¥æœŸæ ¼å¼è½¬æ¢ï¼š- -> /
            if '-' in rel_id and rel_id.count('-') >= 2:
                normalized = rel_id.replace('-', '/')
                if normalized in entity_ids:
                    self.id_map[rel_id] = normalized
                    self.fix_stats['date_dash_to_slash'] += 1
                    continue

            # å°è¯•ä¸‹åˆ’çº¿è½¬æ¢ï¼š_ -> -
            if '_' in rel_id:
                normalized = rel_id.replace('_', '-')
                if normalized in entity_ids:
                    self.id_map[rel_id] = normalized
                    self.fix_stats['underscore_to_dash'] += 1
                    continue

            # å°è¯•ä¸‹åˆ’çº¿è½¬æ¢ï¼š- -> _
            if '-' in rel_id:
                normalized = rel_id.replace('-', '_')
                if normalized in entity_ids:
                    self.id_map[rel_id] = normalized
                    self.fix_stats['dash_to_underscore'] += 1
                    continue

            # å°è¯•ç»„åˆè½¬æ¢ï¼š_/ -> -_
            if '/' in rel_id and '_' in rel_id:
                normalized = rel_id.replace('/', '_')
                if normalized in entity_ids:
                    self.id_map[rel_id] = normalized
                    self.fix_stats['combined_1'] += 1
                    continue

            # æœªæ‰¾åˆ°åŒ¹é…
            self.fix_stats['unmatched'] += 1
            self.id_map[rel_id] = rel_id  # ä¿æŒåŸæ ·

        logger.info(f"IDæ˜ å°„è¡¨æ„å»ºå®Œæˆ: {len(self.id_map)} ä¸ªæ¡ç›®")

    def _analyze_matches(self) -> Tuple[int, int, List[str]]:
        """åˆ†æåŒ¹é…æƒ…å†µ"""
        logger.info("åˆ†æIDåŒ¹é…æƒ…å†µ...")

        # æ”¶é›†æ‰€æœ‰å®ä½“primary_id
        entity_ids = set()
        for entity in self.entities:
            props = entity.get('properties', {})
            primary_id = props.get('primary_id', '')
            if primary_id:
                entity_ids.add(primary_id)

        # æ£€æŸ¥ä¿®å¤å‰çš„åŒ¹é…æƒ…å†µ
        before_from_match = 0
        before_to_match = 0
        unmatched_samples = []

        for rel in self.relationships:
            from_id = rel.get('from', '')
            to_id = rel.get('to', '')

            if from_id in entity_ids:
                before_from_match += 1
            elif len(unmatched_samples) < 20:
                unmatched_samples.append(f"FROM: {from_id}")

            if to_id in entity_ids:
                before_to_match += 1
            elif len(unmatched_samples) < 20:
                unmatched_samples.append(f"TO: {to_id}")

        total_rels = len(self.relationships)
        logger.info(f"\nä¿®å¤å‰:")
        logger.info(f"  From IDåŒ¹é…: {before_from_match}/{total_rels} ({100*before_from_match/total_rels:.1f}%)")
        logger.info(f"  To IDåŒ¹é…: {before_to_match}/{total_rels} ({100*before_to_match/total_rels:.1f}%)")

        return before_from_match, before_to_match, unmatched_samples

    def fix_relationships(self) -> List[Dict]:
        """ä¿®å¤å…³ç³»ID"""
        logger.info("\nå¼€å§‹ä¿®å¤å…³ç³»ID...")

        # å…ˆåˆ†æ
        before_from, before_to, _ = self._analyze_matches()

        # æ„å»ºæ˜ å°„
        self._build_id_mapping()

        # ä¿®å¤å…³ç³»
        fixed_relationships = []
        from_fixed = 0
        to_fixed = 0

        for rel in self.relationships:
            from_id = rel.get('from', '')
            to_id = rel.get('to', '')

            new_from = self.id_map.get(from_id, from_id)
            new_to = self.id_map.get(to_id, to_id)

            if new_from != from_id:
                from_fixed += 1
            if new_to != to_id:
                to_fixed += 1

            fixed_relationships.append({
                **rel,
                'from': new_from,
                'to': new_to
            })

        logger.info(f"\nä¿®å¤ç»Ÿè®¡:")
        logger.info(f"  From IDä¿®å¤: {from_fixed}")
        logger.info(f"  To IDä¿®å¤: {to_fixed}")
        logger.info(f"  æœªå˜åŒ–: {len(self.relationships) - from_fixed - to_fixed}")

        # éªŒè¯ä¿®å¤å
        entity_ids = set()
        for entity in self.entities:
            props = entity.get('properties', {})
            primary_id = props.get('primary_id', '')
            if primary_id:
                entity_ids.add(primary_id)

        after_from_match = sum(1 for r in fixed_relationships if r.get('from', '') in entity_ids)
        after_to_match = sum(1 for r in fixed_relationships if r.get('to', '') in entity_ids)

        logger.info(f"\nä¿®å¤å:")
        logger.info(f"  From IDåŒ¹é…: {after_from_match}/{len(fixed_relationships)} ({100*after_from_match/len(fixed_relationships):.1f}%)")
        logger.info(f"  To IDåŒ¹é…: {after_to_match}/{len(fixed_relationships)} ({100*after_to_match/len(fixed_relationships):.1f}%)")

        return fixed_relationships

    def save_results(self, fixed_relationships: List[Dict], output_dir: Path) -> None:
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜ä¿®å¤åçš„å…³ç³»
        rel_output = output_dir / f'relationships_fixed_{timestamp}.json'
        with open(rel_output, 'w', encoding='utf-8') as f:
            json.dump(fixed_relationships, f, ensure_ascii=False, indent=2)
        logger.info(f"\nâœ… ä¿®å¤åçš„å…³ç³»å·²ä¿å­˜: {rel_output}")

        # ä¿å­˜ç»Ÿè®¡
        stats = {
            'timestamp': timestamp,
            'input_files': {
                'entities': str(self.entities_file),
                'relationships': str(self.relationships_file)
            },
            'entities_count': len(self.entities),
            'relationships_count': len(self.relationships),
            'fix_statistics': dict(self.fix_stats),
            'id_map_size': len(self.id_map)
        }

        stats_output = output_dir / f'crl_fix_statistics_{timestamp}.json'
        with open(stats_output, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“Š ä¿®å¤ç»Ÿè®¡å·²ä¿å­˜: {stats_output}")

        # ä¿å­˜è¯¦ç»†åˆ†æ
        entity_ids = set()
        for entity in self.entities:
            props = entity.get('properties', {})
            primary_id = props.get('primary_id', '')
            if primary_id:
                entity_ids.add(primary_id)

        analysis = {
            'total_relationships': len(self.relationships),
            'id_map_size': len(self.id_map),
            'fix_statistics': dict(self.fix_stats),
            'unmatched_relationship_ids': [],
            'unmatched_entity_ids': []
        }

        # æ‰¾å‡ºæœªåŒ¹é…çš„å…³ç³»ID
        for rel in fixed_relationships[:50]:  # åªè®°å½•å‰50ä¸ª
            from_id = rel.get('from', '')
            to_id = rel.get('to', '')
            if from_id and from_id not in entity_ids:
                analysis['unmatched_relationship_ids'].append({'role': 'from', 'id': from_id})
            if to_id and to_id not in entity_ids:
                analysis['unmatched_relationship_ids'].append({'role': 'to', 'id': to_id})

        analysis_output = output_dir / f'crl_id_mapping_analysis_{timestamp}.json'
        with open(analysis_output, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ” è¯¦ç»†åˆ†æå·²ä¿å­˜: {analysis_output}")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 80)
    logger.info("PharmaKG CRL IDæ˜ å°„ä¿®å¤")
    logger.info("=" * 80)

    # æ•°æ®ç›®å½•
    data_root = Path('/root/autodl-tmp/pj-pharmaKG/data')
    crl_dir = data_root / 'processed' / 'documents' / 'clinical_crl'
    log_dir = data_root / 'logs'
    log_dir.mkdir(exist_ok=True)

    # æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®æ–‡ä»¶
    entity_files = sorted(crl_dir.glob('entities_*.json'), reverse=True)
    rel_files = sorted(crl_dir.glob('relationships_*.json'), reverse=True)

    if not entity_files or not rel_files:
        logger.error("æœªæ‰¾åˆ°CRLæ•°æ®æ–‡ä»¶")
        return

    # è·³è¿‡å·²ä¿®å¤çš„æ–‡ä»¶
    entity_files = [f for f in entity_files if 'fixed' not in f.name]
    rel_files = [f for f in rel_files if 'fixed' not in f.name]

    if not entity_files or not rel_files:
        logger.warning("æœªæ‰¾åˆ°æœªä¿®å¤çš„CRLæ•°æ®æ–‡ä»¶")
        return

    entities_file = entity_files[0]
    relationships_file = rel_files[0]

    logger.info(f"\nè¾“å…¥æ–‡ä»¶:")
    logger.info(f"  å®ä½“: {entities_file.name}")
    logger.info(f"  å…³ç³»: {relationships_file.name}")

    # åˆ›å»ºä¿®å¤å™¨å¹¶æ‰§è¡Œ
    fixer = CRLIDMappingFixer(entities_file, relationships_file)
    fixed_relationships = fixer.fix_relationships()
    fixer.save_results(fixed_relationships, crl_dir)

    logger.info("\n" + "=" * 80)
    logger.info("âœ… CRL IDæ˜ å°„ä¿®å¤å®Œæˆ")
    logger.info("=" * 80)


if __name__ == '__main__':
    main()
