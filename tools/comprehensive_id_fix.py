#!/usr/bin/env python3
#===========================================================
# PharmaKG ç»¼åˆIDæ˜ å°„ä¿®å¤å·¥å…·
# Pharmaceutical Knowledge Graph - Comprehensive ID Mapping Fix
#===========================================================
# ç‰ˆæœ¬: v1.0
# æè¿°: ä¿®å¤æ‰€æœ‰æ•°æ®æºçš„IDæ˜ å°„é—®é¢˜ï¼ŒåŒ…æ‹¬ç‰¹æ®Šå­—ç¬¦å¤„ç†
#===========================================================

import json
import logging
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s: %(message)s'
)
logger = logging.getLogger(__name__)


def normalize_id_for_comparison(id_str: str) -> str:
    """
    æ ‡å‡†åŒ–IDç”¨äºæ¯”è¾ƒ
    - ç§»é™¤æ‰€æœ‰éå­—æ¯æ•°å­—å­—ç¬¦ï¼ˆä¿ç•™ä¸‹åˆ’çº¿ï¼‰
    - å°†æ–œæ è½¬ä¸ºçŸ­æ¨ªçº¿
    - å°†è¿ç»­çš„çŸ­æ¨ªçº¿/ä¸‹åˆ’çº¿å‹ç¼©ä¸ºå•ä¸ª
    - è½¬ä¸ºå°å†™
    """
    if not id_str:
        return id_str

    # æ›¿æ¢æ–œæ ä¸ºçŸ­æ¨ªçº¿
    normalized = id_str.replace('/', '-')

    # ç§»é™¤æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™å­—æ¯æ•°å­—ã€ä¸‹åˆ’çº¿ã€çŸ­æ¨ªçº¿ï¼‰
    normalized = re.sub(r'[^\w\-]', '', normalized)

    # å‹ç¼©è¿ç»­çš„çŸ­æ¨ªçº¿/ä¸‹åˆ’çº¿
    while '--' in normalized:
        normalized = normalized.replace('--', '-')
    while '__' in normalized:
        normalized = normalized.replace('__', '_')
    while '-_' in normalized or '_-' in normalized:
        normalized = normalized.replace('-_', '_').replace('_-', '_')

    return normalized.lower()


def build_fuzzy_id_map(entity_ids: Set[str]) -> Dict[str, str]:
    """
    æ„å»ºæ¨¡ç³ŠIDæ˜ å°„è¡¨

    å¯¹äºæ¯ä¸ªå®ä½“IDï¼Œåˆ›å»ºå¤šä¸ªå˜ä½“æ˜ å°„åˆ°åŸå§‹ID
    """
    id_map = {}

    # ç›´æ¥æ˜ å°„
    for entity_id in entity_ids:
        id_map[entity_id] = entity_id

    # ä¸ºæ¯ä¸ªå®ä½“IDåˆ›å»ºæ ‡å‡†åŒ–å˜ä½“
    for entity_id in entity_ids:
        normalized = normalize_id_for_comparison(entity_id)

        # å¦‚æœæ ‡å‡†åŒ–åçš„IDä¸åŸå§‹ä¸åŒï¼Œæ·»åŠ æ˜ å°„
        if normalized != entity_id.lower():
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ˜ å°„
            if normalized in id_map:
                # å¦‚æœå¤šä¸ªå®ä½“æ˜ å°„åˆ°åŒä¸€ä¸ªæ ‡å‡†åŒ–IDï¼Œä¿ç•™ç¬¬ä¸€ä¸ª
                continue
            id_map[normalized] = entity_id

    return id_map


class ComprehensiveIDFixer:
    """
    ç»¼åˆIDæ˜ å°„ä¿®å¤å™¨
    å¤„ç†æ‰€æœ‰æ•°æ®æºçš„IDæ˜ å°„é—®é¢˜
    """

    def __init__(self, data_root: Path):
        self.data_root = data_root
        self.log_dir = data_root / 'logs'
        self.log_dir.mkdir(exist_ok=True)

    def fix_regulatory_data(self):
        """ä¿®å¤ç›‘ç®¡æ•°æ®"""
        logger.info("\n### ä¿®å¤ç›‘ç®¡æ•°æ® ###")
        reg_dir = self.data_root / 'processed' / 'documents' / 'regulatory'

        # æŸ¥æ‰¾æœ€æ–°çš„å®ä½“æ–‡ä»¶
        entity_files = sorted(reg_dir.glob('entities_fixed_*.json'), reverse=True)
        if not entity_files:
            entity_files = sorted(reg_dir.glob('entities_*.json'), reverse=True)
            entity_files = [f for f in entity_files if 'fixed' not in f.name]

        if not entity_files:
            logger.warning("æœªæ‰¾åˆ°ç›‘ç®¡å®ä½“æ–‡ä»¶")
            return

        entities_file = entity_files[0]
        logger.info(f"å®ä½“æ–‡ä»¶: {entities_file.name}")

        # åŠ è½½å®ä½“
        with open(entities_file, 'r') as f:
            entities = json.load(f)

        # æŸ¥æ‰¾æœ€æ–°çš„å…³ç³»æ–‡ä»¶
        rel_files = sorted(reg_dir.glob('relationships_fixed_*.json'), reverse=True)
        if not rel_files:
            rel_files = sorted(reg_dir.glob('relationships_*.json'), reverse=True)
            rel_files = [f for f in rel_files if 'fixed' not in f.name]

        if not rel_files:
            logger.warning("æœªæ‰¾åˆ°ç›‘ç®¡å…³ç³»æ–‡ä»¶")
            return

        relationships_file = rel_files[0]
        logger.info(f"å…³ç³»æ–‡ä»¶: {relationships_file.name}")

        # åŠ è½½å…³ç³»
        with open(relationships_file, 'r') as f:
            relationships = json.load(f)

        logger.info(f"åŠ è½½ {len(entities)} ä¸ªå®ä½“, {len(relationships)} ä¸ªå…³ç³»")

        # ä¿®å¤ï¼ˆç›‘ç®¡æ•°æ®å·²ç»åœ¨ä¹‹å‰ä¿®å¤è¿‡ï¼Œè¿™é‡Œä¸»è¦æ˜¯éªŒè¯ï¼‰
        self._fix_and_save(entities, relationships, reg_dir, 'regulatory')

    def fix_crl_data(self):
        """ä¿®å¤CRLæ•°æ®"""
        logger.info("\n### ä¿®å¤CRLæ•°æ® ###")
        crl_dir = self.data_root / 'processed' / 'documents' / 'clinical_crl'

        # æŸ¥æ‰¾æœ€æ–°çš„å®ä½“æ–‡ä»¶
        entity_files = sorted(crl_dir.glob('entities_*.json'), reverse=True)
        entity_files = [f for f in entity_files if 'fixed' not in f.name]

        if not entity_files:
            logger.warning("æœªæ‰¾åˆ°CRLå®ä½“æ–‡ä»¶")
            return

        entities_file = entity_files[0]
        logger.info(f"å®ä½“æ–‡ä»¶: {entities_file.name}")

        # åŠ è½½å®ä½“
        with open(entities_file, 'r') as f:
            entities = json.load(f)

        # æŸ¥æ‰¾æœ€æ–°çš„å…³ç³»æ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯ä¿®å¤è¿‡çš„ï¼‰
        rel_files = sorted(crl_dir.glob('relationships_*.json'), reverse=True)
        rel_files = [f for f in rel_files if 'summary' not in f.name]

        if not rel_files:
            logger.warning("æœªæ‰¾åˆ°CRLå…³ç³»æ–‡ä»¶")
            return

        relationships_file = rel_files[0]
        logger.info(f"å…³ç³»æ–‡ä»¶: {relationships_file.name}")

        # åŠ è½½å…³ç³»
        with open(relationships_file, 'r') as f:
            relationships = json.load(f)

        logger.info(f"åŠ è½½ {len(entities)} ä¸ªå®ä½“, {len(relationships)} ä¸ªå…³ç³»")

        # ä¿®å¤
        self._fix_and_save(entities, relationships, crl_dir, 'crl')

    def _fix_and_save(self, entities: List[Dict], relationships: List[Dict], output_dir: Path, data_type: str):
        """ä¿®å¤å¹¶ä¿å­˜"""
        # æ”¶é›†å®ä½“ID
        entity_ids = set()
        entity_by_normalized = {}

        for entity in entities:
            props = entity.get('properties', {})
            primary_id = props.get('primary_id', '')
            if primary_id:
                entity_ids.add(primary_id)
                normalized = normalize_id_for_comparison(primary_id)
                entity_by_normalized[normalized] = primary_id

        logger.info(f"å®ä½“IDæ•°é‡: {len(entity_ids)}")
        logger.info(f"æ ‡å‡†åŒ–å®ä½“IDæ•°é‡: {len(entity_by_normalized)}")

        # ä¿®å¤å…³ç³»
        fixed_relationships = []
        fix_stats = defaultdict(int)

        for rel in relationships:
            from_id = rel.get('from', '')
            to_id = rel.get('to', '')

            # å°è¯•ç›´æ¥åŒ¹é…
            new_from = from_id if from_id in entity_ids else None
            new_to = to_id if to_id in entity_ids else None

            # å°è¯•æ ‡å‡†åŒ–åŒ¹é…
            if new_from is None:
                normalized_from = normalize_id_for_comparison(from_id)
                new_from = entity_by_normalized.get(normalized_from, from_id)
                if new_from != from_id:
                    fix_stats['from_normalized'] += 1

            if new_to is None:
                normalized_to = normalize_id_for_comparison(to_id)
                new_to = entity_by_normalized.get(normalized_to, to_id)
                if new_to != to_id:
                    fix_stats['to_normalized'] += 1

            # æ£€æŸ¥æ˜¯å¦ä»ä¸åŒ¹é…
            if new_from not in entity_ids:
                fix_stats['from_unmatched'] += 1
            if new_to not in entity_ids:
                fix_stats['to_unmatched'] += 1

            fixed_relationships.append({
                **rel,
                'from': new_from,
                'to': new_to
            })

        # éªŒè¯ä¿®å¤ç»“æœ
        after_from_match = sum(1 for r in fixed_relationships if r.get('from', '') in entity_ids)
        after_to_match = sum(1 for r in fixed_relationships if r.get('to', '') in entity_ids)

        logger.info(f"\nä¿®å¤ç»Ÿè®¡:")
        logger.info(f"  From IDæ ‡å‡†åŒ–: {fix_stats['from_normalized']}")
        logger.info(f"  To IDæ ‡å‡†åŒ–: {fix_stats['to_normalized']}")
        logger.info(f"  From IDæœªåŒ¹é…: {fix_stats['from_unmatched']}")
        logger.info(f"  To IDæœªåŒ¹é…: {fix_stats['to_unmatched']}")
        logger.info(f"\nä¿®å¤å:")
        logger.info(f"  From IDåŒ¹é…: {after_from_match}/{len(relationships)} ({100*after_from_match/len(relationships):.1f}%)")
        logger.info(f"  To IDåŒ¹é…: {after_to_match}/{len(relationships)} ({100*after_to_match/len(relationships):.1f}%)")

        # ä¿å­˜ä¿®å¤åçš„å…³ç³»
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = output_dir / f'relationships_fixed_{timestamp}.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(fixed_relationships, f, ensure_ascii=False, indent=2)
        logger.info(f"\nâœ… ä¿®å¤åçš„å…³ç³»å·²ä¿å­˜: {output_file.name}")

        # ä¿å­˜åˆ†æ
        analysis = {
            'data_type': data_type,
            'timestamp': timestamp,
            'entities_count': len(entities),
            'relationships_count': len(relationships),
            'fix_stats': dict(fix_stats),
            'from_match_rate': 100 * after_from_match / len(relationships),
            'to_match_rate': 100 * after_to_match / len(relationships)
        }

        analysis_file = self.log_dir / f'{data_type}_id_fix_analysis_{timestamp}.json'
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“Š åˆ†æå·²ä¿å­˜: {analysis_file.name}")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 80)
    logger.info("PharmaKG ç»¼åˆIDæ˜ å°„ä¿®å¤")
    logger.info("=" * 80)

    data_root = Path('/root/autodl-tmp/pj-pharmaKG/data')
    fixer = ComprehensiveIDFixer(data_root)

    # ä¿®å¤ç›‘ç®¡æ•°æ®
    fixer.fix_regulatory_data()

    # ä¿®å¤CRLæ•°æ®
    fixer.fix_crl_data()

    logger.info("\n" + "=" * 80)
    logger.info("âœ… ç»¼åˆIDæ˜ å°„ä¿®å¤å®Œæˆ")
    logger.info("=" * 80)


if __name__ == '__main__':
    main()
